{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building makemore: Multi-layer perceptron\n",
    "\n",
    "https://www.youtube.com/watch?v=TCH_1BHY58I\n",
    "\n",
    "Last time: two models, the first using counts and normalizing them to generate the next character in a sequence. Problem: using more characters grows exponentially. With the stop character, $27^n$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benigo et al. approach\n",
    "\n",
    "Better approach: multi-layer perceptron to predict future characters. Scales much better. See: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "\n",
    "Goes from 17,000 feature vectors to a 30-dimension space (dimensionality reduction). Approach otherwise is as seen in the previous lecture.\n",
    "\n",
    "Intuitively, \"walking\" and \"running\" should be close to one another in the model space, so if the model's never seen \"The dog was running in the ___\" but it's seen \"The cat was walking in the bedroom\" it'll consider bedroom a likely word to fill in the blank. (Sounds like word embeddings.)\n",
    "\n",
    "Network architecture: embedded input layers, hidden layer (30 parameters), output layer (17,000 parameters). Use softmax to take most probable words when generating a sequence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "words = open('makemore/names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary - same approach as last lecture, identical code\n",
    "# Put special . as 0 element, and shift the alphabet over by 1\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... --> e\n",
      "..e --> m\n",
      ".em --> m\n",
      "emm --> a\n",
      "mma --> .\n",
      "olivia\n",
      "... --> o\n",
      "..o --> l\n",
      ".ol --> i\n",
      "oli --> v\n",
      "liv --> i\n",
      "ivi --> a\n",
      "via --> .\n",
      "ava\n",
      "... --> a\n",
      "..a --> v\n",
      ".av --> a\n",
      "ava --> .\n",
      "isabella\n",
      "... --> i\n",
      "..i --> s\n",
      ".is --> a\n",
      "isa --> b\n",
      "sab --> e\n",
      "abe --> l\n",
      "bel --> l\n",
      "ell --> a\n",
      "lla --> .\n",
      "sophia\n",
      "... --> s\n",
      "..s --> o\n",
      ".so --> p\n",
      "sop --> h\n",
      "oph --> i\n",
      "phi --> a\n",
      "hia --> .\n"
     ]
    }
   ],
   "source": [
    "# Build dataset - new approach using block_size to use the previous n characters to predict the next\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "X, Y = [], [] # input, and labels, to neural network\n",
    "\n",
    "for w in words[:5]: # Test on just first few names\n",
    "    print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(''.join(itos[i] for i in context), '-->', itos[ix])\n",
    "        context = context[1:] + [ix] # crop and append\n",
    "\n",
    "# Save results\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 examples can be generated: the 4 letters and the stop character. Note that initially there is no previous character, then e, em, emm, mma. The block size is a rolling window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper had 17,000 words, while we have 27 characters (a-z and .) so we need a dimension <27, 2> lookup table.\n",
    "\n",
    "Conceptually, we use a one-hot encoded vector (indicator) to pull out the desired row from the lookup matrix as before. But this time it'll be indexed because this is faster to look up values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4713,  0.7868])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator = g)\n",
    "F.one_hot(torch.tensor(5), num_classes = 27).float() @ C"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding a single integer: easy, just return e.g. `C[5]`. Works the same way for tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4713,  0.7868],\n",
       "        [-0.3284, -0.4330],\n",
       "        [ 1.3729,  2.9334],\n",
       "        [ 1.3729,  2.9334],\n",
       "        [ 1.3729,  2.9334]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[torch.tensor([5, 6, 7, 7, 7])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0274, -1.1008])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X][4,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden layer\n",
    "\n",
    "Weights are initialized randomly as usual; it'll be 3 * 2 = 6 inputs, due to 2-dimensional embeddings * 3 block size. The number of neurons is up to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((6, 100), generator = g)\n",
    "b1 = torch.randn(100, generator = g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`emb @ W1 + b1` doesn't work due to dimensionality of emb. Need to concatenate the 3, 2 into 6 to do matrix multiplication with `W1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 2]), torch.Size([32, 2]), torch.Size([32, 2]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use torch.cat to concatenate the embeddings for each imput.\n",
    "emb[:, 0, :].shape, emb[:, 1, :].shape, emb[:, 2, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate on the 1st indexed dimension\n",
    "torch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]], 1).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has the correct dimensions; just needs to be updated to be generalizable based on the block size. Use `torch.unbind` to unwrap the tensor along a given dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(torch.unbind(emb, 1), 1).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be done even more efficiently by recasting the dimensions of a tensor directly by using `view`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(18)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1],\n",
       "        [ 2,  3],\n",
       "        [ 4,  5],\n",
       "        [ 6,  7],\n",
       "        [ 8,  9],\n",
       "        [10, 11],\n",
       "        [12, 13],\n",
       "        [14, 15],\n",
       "        [16, 17]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(9, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5]],\n",
       "\n",
       "        [[ 6,  7],\n",
       "         [ 8,  9],\n",
       "         [10, 11]],\n",
       "\n",
       "        [[12, 13],\n",
       "         [14, 15],\n",
       "         [16, 17]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(3, 3, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as the arguments multiply to 18 in this case, the sequence can be represented with the given dimensions without any additional memory being used. Very efficient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5674, -0.2373,  1.5674, -0.2373,  1.5674, -0.2373],\n",
       "        [ 1.5674, -0.2373,  1.5674, -0.2373, -0.4713,  0.7868],\n",
       "        [ 1.5674, -0.2373, -0.4713,  0.7868,  2.4448, -0.6701],\n",
       "        [-0.4713,  0.7868,  2.4448, -0.6701,  2.4448, -0.6701],\n",
       "        [ 2.4448, -0.6701,  2.4448, -0.6701, -0.0274, -1.1008],\n",
       "        [ 1.5674, -0.2373,  1.5674, -0.2373,  1.5674, -0.2373],\n",
       "        [ 1.5674, -0.2373,  1.5674, -0.2373, -1.0725,  0.7276],\n",
       "        [ 1.5674, -0.2373, -1.0725,  0.7276, -0.0707,  2.4968],\n",
       "        [-1.0725,  0.7276, -0.0707,  2.4968,  0.6772, -0.8404],\n",
       "        [-0.0707,  2.4968,  0.6772, -0.8404, -0.1158, -1.2078],\n",
       "        [ 0.6772, -0.8404, -0.1158, -1.2078,  0.6772, -0.8404],\n",
       "        [-0.1158, -1.2078,  0.6772, -0.8404, -0.0274, -1.1008],\n",
       "        [ 1.5674, -0.2373,  1.5674, -0.2373,  1.5674, -0.2373],\n",
       "        [ 1.5674, -0.2373,  1.5674, -0.2373, -0.0274, -1.1008],\n",
       "        [ 1.5674, -0.2373, -0.0274, -1.1008, -0.1158, -1.2078],\n",
       "        [-0.0274, -1.1008, -0.1158, -1.2078, -0.0274, -1.1008],\n",
       "        [ 1.5674, -0.2373,  1.5674, -0.2373,  1.5674, -0.2373],\n",
       "        [ 1.5674, -0.2373,  1.5674, -0.2373,  0.6772, -0.8404],\n",
       "        [ 1.5674, -0.2373,  0.6772, -0.8404,  0.1476, -1.0006],\n",
       "        [ 0.6772, -0.8404,  0.1476, -1.0006, -0.0274, -1.1008],\n",
       "        [ 0.1476, -1.0006, -0.0274, -1.1008,  0.2859, -0.0296],\n",
       "        [-0.0274, -1.1008,  0.2859, -0.0296, -0.4713,  0.7868],\n",
       "        [ 0.2859, -0.0296, -0.4713,  0.7868, -0.0707,  2.4968],\n",
       "        [-0.4713,  0.7868, -0.0707,  2.4968, -0.0707,  2.4968],\n",
       "        [-0.0707,  2.4968, -0.0707,  2.4968, -0.0274, -1.1008],\n",
       "        [ 1.5674, -0.2373,  1.5674, -0.2373,  1.5674, -0.2373],\n",
       "        [ 1.5674, -0.2373,  1.5674, -0.2373,  0.1476, -1.0006],\n",
       "        [ 1.5674, -0.2373,  0.1476, -1.0006, -1.0725,  0.7276],\n",
       "        [ 0.1476, -1.0006, -1.0725,  0.7276,  0.0511,  1.3095],\n",
       "        [-1.0725,  0.7276,  0.0511,  1.3095,  1.5618, -1.6261],\n",
       "        [ 0.0511,  1.3095,  1.5618, -1.6261,  0.6772, -0.8404],\n",
       "        [ 1.5618, -1.6261,  0.6772, -0.8404, -0.0274, -1.1008]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same as concatenation before\n",
    "emb.view(32, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5071,  3.5656, -2.4451,  ...,  2.5479, -0.8539,  3.7077],\n",
       "        [-4.2930,  6.2880, -5.4224,  ...,  2.6233, -1.7728,  3.3300],\n",
       "        [-2.5582, -0.1837,  1.6790,  ...,  2.6169, -0.0838,  2.6059],\n",
       "        ...,\n",
       "        [ 1.8194, -4.3532,  5.9360,  ...,  2.4439,  0.0743, -3.7715],\n",
       "        [ 0.7255,  6.5159,  2.6143,  ...,  2.2033, -0.4564,  1.9066],\n",
       "        [-3.7433,  3.3581, -2.3161,  ...,  0.6154, -1.1615,  4.6473]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.view(32, 6) @ W1 + b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5071,  3.5656, -2.4451,  ...,  2.5479, -0.8539,  3.7077],\n",
       "        [-4.2930,  6.2880, -5.4224,  ...,  2.6233, -1.7728,  3.3300],\n",
       "        [-2.5582, -0.1837,  1.6790,  ...,  2.6169, -0.0838,  2.6059],\n",
       "        ...,\n",
       "        [ 1.8194, -4.3532,  5.9360,  ...,  2.4439,  0.0743, -3.7715],\n",
       "        [ 0.7255,  6.5159,  2.6143,  ...,  2.2033, -0.4564,  1.9066],\n",
       "        [-3.7433,  3.3581, -2.3161,  ...,  0.6154, -1.1615,  4.6473]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Even better, have pytorch infer the dimensions\n",
    "emb.view(-1, 6) @ W1 + b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "h.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `+ b1` which is a constant bias term; due to Torch's internals even though b1 is a constant vector it will be copied down and added to all rows correctly; always worth checking this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output layer\n",
    "\n",
    "Input is 100 neurons, output is 27 possible characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100, 27), generator = g)\n",
    "b2 = torch.randn(27, generator = g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = h @ W2 + b2\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdims = True) # Sum along first dimension\n",
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[0].sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows sum to 1.\n",
    "\n",
    "Still need to grab probabilities and index prob, and compare to Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.2200e-08, 1.1838e-04, 2.8906e-09, 2.6372e-12, 1.3740e-07, 4.0513e-22,\n",
       "        3.9412e-06, 1.7937e-07, 9.5025e-11, 2.0370e-06, 3.2776e-09, 6.3124e-07,\n",
       "        6.4844e-16, 3.7328e-06, 1.1113e-05, 5.4767e-07, 1.1513e-14, 3.1438e-14,\n",
       "        7.4242e-09, 1.7710e-09, 5.0028e-01, 7.0436e-09, 1.7890e-11, 3.5921e-14,\n",
       "        3.4840e-15, 8.1048e-17, 1.3126e-16, 1.1197e-07, 3.1022e-11, 1.4176e-08,\n",
       "        5.4811e-09, 5.7530e-04])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[torch.arange(32), Y]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative log likelihood is defined as before, and will be minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(21.1617)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaned up code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.Size([32]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator = g)\n",
    "W1 = torch.randn((6, 100), generator = g)\n",
    "b1 = torch.randn(100, generator = g)\n",
    "W2 = torch.randn((100, 27), generator = g)\n",
    "b2 = torch.randn(27, generator = g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # Total parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should be the same as when we multiply out the tensor sizes\n",
    "27 * 2 + 6 * 100 + 100 + 100 * 27 + 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss function with current parameters\n",
    "emb = C[X] # (32, 3, 2)\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdims = True)\n",
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can be further optimized with the `corss_entropy` function which takes the place of the `counts =`, `prob =`, `loss =`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss function with current parameters\n",
    "emb = C[X] # (32, 3, 2)\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "#counts = logits.exp()\n",
    "#prob = counts / counts.sum(1, keepdims = True)\n",
    "#loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss # Should be same as above"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always best to use the built-in operations like `cross_entropy` because intermediate steps aren't saved. It's also a better, more efficient implementation for the backward step. Can have a simpler expression when calculating the backpropagation gradients and updates.\n",
    "\n",
    "Additionally, `cross_entropy` can avoid various numerical issues. Consider what happens with extreme values when you roll your own implementation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 3.3311e-04, 6.6906e-03, 9.9298e-01])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.tensor([-100, -3, 0, 5])\n",
    "counts = logits.exp()\n",
    "counts / counts.sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works ok with lower values, but leads to numerical errors with large numbers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., nan])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.tensor([-100, -3, 0, 100]) # But large numbers lead to numerical errors\n",
    "counts = logits.exp() # ...due to exp() which blows up with large positive numbers\n",
    "counts / counts.sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, Pytorch subtracts the maximum from the entire vector to ensure nothing is > 0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
