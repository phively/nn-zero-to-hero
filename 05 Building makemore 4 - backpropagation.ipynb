{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building makemore: becoming a backprop ninja\n",
    "\n",
    "Before going to more complicated networks, one more lecture to really understand the backpropagation step. Can't just automatically stack together differentiable functions and expect it to work; can shoot ourselves in the foot if internal mechanisms aren't understood. Need to understand under the hood to debug it.\n",
    "\n",
    "Flat tails like tanh and sigmoid lead to gradient issues (vanishing gradients), dead neurons, and other problems we saw before.\n",
    "\n",
    "We covered autograd in the micrograd lecture; good exercise to redo in the language of tensors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
